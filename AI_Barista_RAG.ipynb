{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFqkYQjuEi+AqAmK6Sq0LH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "076cc8905f714cfcb69a48fa76798432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Input:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_6ace9974ff5942099180195873f64a0c",
            "placeholder": "Enter your input",
            "style": "IPY_MODEL_7fa995d7a0ed4806bbe029af7b780902",
            "value": "I want a caramel cappucinno with a sweet punch"
          }
        },
        "6ace9974ff5942099180195873f64a0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fa995d7a0ed4806bbe029af7b780902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c326fe28fde04f27af9661d9ea251ec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7f40ba7fa44946a7a850570267d3d80d",
            "style": "IPY_MODEL_1f3d1ccd4927440c91dc532b44cd4f46",
            "tooltip": ""
          }
        },
        "7f40ba7fa44946a7a850570267d3d80d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f3d1ccd4927440c91dc532b44cd4f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "2aec0bd274174e26b31deff3e915106b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_45eef17b139149d7bd4249c10f870bd1",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Drink: Caramel Cappuccino with a Sweet Punch\n",
                  "Size: 12oz\n",
                  "Ingredients:\n",
                  "  - 2 shots of espresso\n",
                  "  - 20 ml of caramel syrup\n",
                  "  - 10 ml of vanilla syrup\n",
                  "  - 10 ml of raspberry syrup\n",
                  "  - 150 ml of whole milk\n"
                ]
              }
            ]
          }
        },
        "45eef17b139149d7bd4249c10f870bd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darthgera123/RAG-Agents/blob/main/AI_Barista_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AI Barista\n",
        "In this example we want to build an AI agent that can produce detailed instructions for making coffee. We need to create a toy database with some attributes and the agent will match the text input with the toy database that we have created.\n",
        "\n",
        "We leverage OpenAI API for understanding queries. We embed our toy database and the querry in the same space and then prompt the LLM to answer the question. The objective is to use the LLM reasoning capabilities to parse the provided database for the user query.\n"
      ],
      "metadata": {
        "id": "L2XTPYbYcZCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Codebase\n",
        "\n",
        "## Install Libraries"
      ],
      "metadata": {
        "id": "B_13kDtbi3ol"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "30o4j18WRfl3",
        "outputId": "959a2a6f-c819-4ecb-c711-aa7f70fb7633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.13)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai fuzzywuzzy[speedup]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries and Add API Keys"
      ],
      "metadata": {
        "id": "HtXrX4oUi7FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "openai.api_key = userdata.get('OPEN_AI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_AI_API_KEY')\n",
        "client = openai.Client()"
      ],
      "metadata": {
        "id": "PQJh6834g9Ut"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Database\n",
        "We create a simple list of ingredients consisting of names and properties. The properties would help us guide what ingredient the user is looking for. We include variants of syrups and milks"
      ],
      "metadata": {
        "id": "8d_o0ItmjMy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of ingredients with their properties\n",
        "ingredients = [\n",
        "    {\"name\": \"caramel syrup\", \"properties\": \"sweet and rich\"},\n",
        "    {\"name\": \"raspberry syrup\", \"properties\": \"fruity and tart\"},\n",
        "    {\"name\": \"cinnamon\", \"properties\": \"warm and spicy\"},\n",
        "    {\"name\": \"lemon zest\", \"properties\": \"citrusy and tangy\"},\n",
        "    {\"name\": \"vanilla syrup\", \"properties\": \"sweet and creamy\"},\n",
        "    {\"name\": \"whole milk\", \"properties\": \"rich and creamy\"},\n",
        "    {\"name\": \"skim milk\", \"properties\": \"light and low-fat\"},\n",
        "    {\"name\": \"almond milk\", \"properties\": \"nutty and dairy-free\"},\n",
        "    {\"name\": \"oat milk\", \"properties\": \"creamy and dairy-free\"},\n",
        "    {\"name\": \"soy milk\", \"properties\": \"smooth and dairy-free\"}\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bjecusPnh6k4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We leverage OpenAI's API to embed the database which we will later use for query."
      ],
      "metadata": {
        "id": "6J4OxMtPjyic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text):\n",
        "    \"\"\"\n",
        "    Get the embedding for a given text using OpenAI's API.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text for which to get the embedding.\n",
        "\n",
        "    Returns:\n",
        "        list: The embedding for the given text.\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        input=text,\n",
        "        model=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# Generate embeddings for each ingredient's properties\n",
        "for ingredient in ingredients:\n",
        "    ingredient['embedding'] = get_embedding(ingredient['properties'])\n",
        "\n",
        "# Convert the ingredients list to a dictionary for easier access\n",
        "ingredients_db = {ingredient['name']: ingredient for ingredient in ingredients}\n",
        "\n",
        "# Save the database to a file\n",
        "with open('/content/ingredients_db.json', 'w') as f:\n",
        "    json.dump(ingredients_db, f)"
      ],
      "metadata": {
        "id": "qfes-K_3jlgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "To leverage the reasoning capabilities of LLM, we need to define the prompts very carefully.\n",
        "\n",
        "## User Prompt\n",
        "This is the prompt provided by the user. If no prompt is provided we then use a basic predictive model to return the coffee order"
      ],
      "metadata": {
        "id": "M4QU0oK7oCKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def default_request():\n",
        "    \"\"\"\n",
        "    Get user input for default request.\n",
        "\n",
        "    Returns:\n",
        "        str: Output of the predictive model trained\n",
        "    \"\"\"\n",
        "    print(\"Enter details\")\n",
        "    age = input(\"Enter your age: \")\n",
        "    country = input(\"Enter your country: \")\n",
        "    occupation = input(\"Enter your occupation: \")\n",
        "    gender = input(\"Enter your gender: \")\n",
        "    temperature = input(\"Enter your preferred temperature: \")\n",
        "    # return predicted model. add from part 1\n",
        "\n",
        "def get_user_input():\n",
        "    \"\"\"\n",
        "    Get user input for the drink request.\n",
        "\n",
        "    Returns:\n",
        "        str: Output of the RAG agent\n",
        "    \"\"\"\n",
        "    user_input = input(\"Enter your drink request: \")\n",
        "    if not user_input:\n",
        "        user_input = default_request()\n",
        "    return user_input"
      ],
      "metadata": {
        "id": "j5nDvtYGiMMm"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instruction Prompt\n",
        "This is the prompt before the prompt that provides information to the LLM how we want the information. In this case we want it in a very specific JSON format which can be further used to make backend apis.\n",
        "\n",
        "We also add some guardrails to guide the LLM prompting so that it does not hallucinate weird details"
      ],
      "metadata": {
        "id": "NQfa6f24pDdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_instruction_prompt():\n",
        "    \"\"\"\n",
        "    Create the instruction prompt for the LLM.\n",
        "\n",
        "    Returns:\n",
        "        str: The instruction prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful assistant that generates coffee drink recipes. Please provide a recipe in JSON format with the following structure and strict guidelines:\n",
        "\n",
        "{{\n",
        "  \"drink\": {{\n",
        "    \"name\": \"string\",            // Name of the drink\n",
        "    \"size\": \"string\",            // Size of the drink (e.g., \"8oz\", \"12oz\", \"16oz\")\n",
        "    \"ingredients\": [\n",
        "      {{\n",
        "        \"name\": \"string\",        // Name of the ingredient (e.g., \"espresso\", \"caramel syrup\", \"whole milk\",\"water\")\n",
        "        \"quantity\": number,      // Quantity of the ingredient\n",
        "        \"unit\": \"string\"         // Unit of the ingredient (e.g., \"shots\", \"ml\")\n",
        "      }}\n",
        "      // Additional ingredients in the same format\n",
        "    ]\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Guidelines:\n",
        "1. The \"name\" field must always be a string describing the drink name.\n",
        "2. The \"size\" field must always be a string representing the drink size.\n",
        "3. The \"ingredients\" array must contain objects with \"name\", \"quantity\", and \"unit\" fields.\n",
        "4. The \"name\" field in ingredients must be a string.\n",
        "5. The \"quantity\" field in ingredients must be a number.\n",
        "6. The \"unit\" field in ingredients must be a string.\n",
        "7. Do not include any additional fields or change the structure of the JSON.\n",
        "8. Ensure that all ingredient names are valid and expected.\n",
        "9. Check that all quantities are within very strict limits:\n",
        "   - For \"shots\": 1 to 5\n",
        "   - For \"ml\": 5 to 200\n",
        "10. If there is an error in the input or format, provide an error message describing the issue.\n",
        "If the input or format is incorrect, the response should be:\n",
        "{{ \"error\": \"Description of the error\" }}\n",
        "\"\"\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "bpnQm_OAXqz9"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assistant Prompt\n",
        "This is for RAG purposes. The idea is that it provides a context to the LLM from which LLM can query and return the output. In this context we ask it to query the database that we have provided"
      ],
      "metadata": {
        "id": "I5w6eZ62pfI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_assistant_prompt(retrieved_ingredients):\n",
        "    \"\"\"\n",
        "    Create the assistant prompt for the LLM.\n",
        "\n",
        "    Args:\n",
        "        retrieved_ingredients (list): List of retrieved ingredients.\n",
        "\n",
        "    Returns:\n",
        "        str: The assistant prompt.\n",
        "    \"\"\"\n",
        "    ingredients_list = ', '.join([item[0] for item in retrieved_ingredients])\n",
        "    return f\"\"\"Define the recipe by only using ingredients from {ingredients_list}\"\"\""
      ],
      "metadata": {
        "id": "C73GiEafiXst"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Response\n",
        "Initialize the model and other parameters to generate text."
      ],
      "metadata": {
        "id": "pyneIrJTp2fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_response(instruction_prompt,prompt,assistant_prompt=''):\n",
        "    \"\"\"\n",
        "    Get the LLM response.\n",
        "\n",
        "    Args:\n",
        "        instruction_prompt (str): Instruction prompt.\n",
        "        prompt (str): User prompt.\n",
        "        assistant_prompt (str): Assistant prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: LLM response.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": instruction_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
        "        ],\n",
        "        max_tokens=350,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5)\n",
        "    return response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "Cl5zyn74gMTY"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic Search\n",
        "We define a function which embeds our query and returns the top k entries in the database closest to our query in the latent space. We match the similarity using cosine similarity metric"
      ],
      "metadata": {
        "id": "JlkGqB8bq2xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(query, ingredients_db, top_k=3):\n",
        "    \"\"\"\n",
        "    Semantic search for ingredients in the database.\n",
        "\n",
        "    Args:\n",
        "        query (str): User query.\n",
        "        ingredients_db (dict): Database of ingredients.\n",
        "        top_k (int): Number of top results to return.\n",
        "\n",
        "    Returns:\n",
        "        list: List of top results.\n",
        "    \"\"\"\n",
        "\n",
        "    query_embedding = get_embedding(query)\n",
        "    similarities = []\n",
        "\n",
        "    for ingredient in ingredients_db.values():\n",
        "        similarity = cosine_similarity([query_embedding], [ingredient['embedding']])\n",
        "        similarities.append((ingredient['name'], similarity[0][0]))\n",
        "\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_k]"
      ],
      "metadata": {
        "id": "P9m7ztiDgiZ6"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define RAG Agent\n",
        "This function works in 3 steps.\n",
        "\n",
        "\n",
        "1.   Perform Semantic Search and retrieve relevant entries from database\n",
        "2.   Generate all the different prompts\n",
        "3. Pass it to the LLM model\n",
        "\n",
        "Returns the JSON response\n"
      ],
      "metadata": {
        "id": "c9_Zt6OurV5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_agent(user_request):\n",
        "    \"\"\"\n",
        "    RAG agent for generating drink recipes.\n",
        "\n",
        "    Args:\n",
        "        user_request (str): User request.\n",
        "\n",
        "    Returns:\n",
        "        str: LLM JSON response.\n",
        "    \"\"\"\n",
        "\n",
        "    retrieved_ingredients = semantic_search(user_request, ingredients_db)\n",
        "    # print(retrieved_ingredients)\n",
        "\n",
        "    instruction_prompt = create_instruction_prompt()\n",
        "    assistant_prompt = create_assistant_prompt(retrieved_ingredients)\n",
        "    prompt = user_request\n",
        "\n",
        "    response = get_llm_response(instruction_prompt,prompt,assistant_prompt)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "zq2YjWqzmSPW"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# user_request = \"I want a medium caramel latte with a sour punch and nutty milk\"\n",
        "# response = rag_agent(user_request)\n",
        "# print(\"RAG Agent Response:\", response)"
      ],
      "metadata": {
        "id": "ZdWWXaFQ19NJ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parse Response\n",
        "The frontend now parses the JSON dict and parses for user. We also do some error handling by adding a fuzzy search on names of ingredients. Invalid responses are handled. Also ensures that invalid measurements are not passed forward. Can add more checks"
      ],
      "metadata": {
        "id": "C_reukxtsOFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of ingredients available\n",
        "valid_ingredients = {\n",
        "    \"espresso\": \"shots\",\n",
        "    \"caramel syrup\": \"ml\",\n",
        "    \"raspberry syrup\": \"ml\",\n",
        "    \"cinnamon\": \"ml\",\n",
        "    \"lemon juice\": \"ml\",\n",
        "    \"vanilla syrup\": \"ml\",\n",
        "    \"whole milk\": \"ml\",\n",
        "    \"skim milk\": \"ml\",\n",
        "    \"almond milk\": \"ml\",\n",
        "    \"oat milk\": \"ml\",\n",
        "    \"soy milk\": \"ml\",\n",
        "    \"water\": \"ml\"\n",
        "}\n",
        "\n",
        "# Set reasonable quantity limits\n",
        "quantity_limits = {\n",
        "    \"shots\": (1, 5),  # 1 to 5 shots of espresso\n",
        "    \"ml\": (1, 300)  # 1ml to 300ml for syrups and milk\n",
        "}\n",
        "from fuzzywuzzy import process\n",
        "def get_closest_match(name, valid_names):\n",
        "    match, score = process.extractOne(name, valid_names)\n",
        "    return match if score >= 80 else None  # Adjust the threshold as needed\n",
        "\n",
        "def validate_and_pretty_print_recipe(json_response):\n",
        "    \"\"\"\n",
        "    Validate and pretty print the recipe.\n",
        "\n",
        "    Args:\n",
        "        json_response (str): LLM JSON response.\n",
        "\n",
        "    Returns:\n",
        "        str: Pretty-printed recipe.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        recipe = json.loads(json_response)\n",
        "\n",
        "        if \"error\" in recipe:\n",
        "            return f\"Error in generating drink recipe: {recipe['error']}\"\n",
        "\n",
        "        pretty_components = []\n",
        "\n",
        "        drink_name = recipe['drink'].get('name', 'Unknown Drink')\n",
        "\n",
        "        drink_size = recipe['drink'].get('size', 'Unknown Size')\n",
        "\n",
        "        pretty_components.append(f\"Drink: {drink_name}\")\n",
        "        pretty_components.append(f\"Size: {drink_size}\")\n",
        "\n",
        "        pretty_components.append(\"Ingredients:\")\n",
        "        for ingredient in recipe['drink']['ingredients']:\n",
        "            name = ingredient.get('name', 'Unknown Ingredient')\n",
        "            quantity = ingredient.get('quantity', 0)\n",
        "            unit = ingredient.get('unit', '')\n",
        "\n",
        "            closest_match = get_closest_match(name, valid_ingredients.keys())\n",
        "            if not closest_match:\n",
        "                raise ValueError(f\"Invalid ingredient: {name}\")\n",
        "\n",
        "            if unit != valid_ingredients[name]:\n",
        "                raise ValueError(f\"Invalid unit for {name}: {unit}\")\n",
        "\n",
        "            min_qty, max_qty = quantity_limits[unit]\n",
        "            if not (min_qty <= quantity <= max_qty):\n",
        "                raise ValueError(f\"Quantity of {name} is out of bounds: {quantity} {unit}\")\n",
        "\n",
        "            pretty_components.append(f\"  - {quantity} {unit} of {name}\")\n",
        "\n",
        "        pretty_recipe = \"\\n\".join(pretty_components)\n",
        "        return pretty_recipe\n",
        "\n",
        "    except (json.JSONDecodeError, KeyError, ValueError) as e:\n",
        "        return f\"Error in generating drink recipe: {str(e)}\""
      ],
      "metadata": {
        "id": "3XrKtFto2MBl"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Order your drink"
      ],
      "metadata": {
        "id": "pZl_CuH7twhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dear customer, what would you like to have today?\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "input_text = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter your input',\n",
        "    description='Input:',\n",
        "    disabled=False\n",
        ")\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_click(b):\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        user_input = input_text.value\n",
        "        response = rag_agent(user_input)\n",
        "        pretty_recipe = validate_and_pretty_print_recipe(response)\n",
        "        print(pretty_recipe)\n",
        "\n",
        "# Create button widget\n",
        "button = widgets.Button(description=\"Submit\")\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "# Display widgets\n",
        "display(input_text, button, output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224,
          "referenced_widgets": [
            "076cc8905f714cfcb69a48fa76798432",
            "6ace9974ff5942099180195873f64a0c",
            "7fa995d7a0ed4806bbe029af7b780902",
            "c326fe28fde04f27af9661d9ea251ec3",
            "7f40ba7fa44946a7a850570267d3d80d",
            "1f3d1ccd4927440c91dc532b44cd4f46",
            "2aec0bd274174e26b31deff3e915106b",
            "45eef17b139149d7bd4249c10f870bd1"
          ]
        },
        "cellView": "form",
        "id": "L2qmKdC3uQ79",
        "outputId": "8147c2e7-53eb-4a24-d160-913c627a4769"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Input:', placeholder='Enter your input')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "076cc8905f714cfcb69a48fa76798432"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Submit', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c326fe28fde04f27af9661d9ea251ec3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2aec0bd274174e26b31deff3e915106b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Takeaways\n",
        "We leverage gpt4 LLM along with our toy dataset to create a RAG agent that can reason natural language queries. Although we have applied several safeguards it still has some issues.\n",
        "\n",
        "\n",
        "*   Queries like a dash of cinnamon is hard to quantify\n",
        "*   Some properties such as low-fat milk might not translate to the type of milk\n",
        "*  Not too much control over exact quantities and also no idea if the proposed recipe is even good\n",
        "* Tradeoff between runtime speed and accuracy between gpt-3.5 and gpt-4 models. For recipes accuracy is more crucial and since our dataset is quite small, hence we use gpt-4 model for now. With a more descriptive dataset, can use a lightweight model as it focusses more on parsing than hallucination\n",
        "\n"
      ],
      "metadata": {
        "id": "hizvGvWAu8is"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bhW2tqnvyRww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}